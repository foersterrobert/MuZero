{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.row_count = 3\n",
    "        self.column_count = 3\n",
    "        self.action_size = self.row_count * self.column_count\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"TicTacToe\"\n",
    "        \n",
    "    def get_initial_state(self):\n",
    "        return np.zeros((self.row_count, self.column_count))\n",
    "    \n",
    "    def get_next_state(self, state, action, player):\n",
    "        row = action // self.column_count\n",
    "        column = action % self.column_count\n",
    "        state[row, column] = player\n",
    "        return state\n",
    "    \n",
    "    def get_valid_moves(self, state):\n",
    "        return (state.reshape(-1) == 0).astype(np.uint8)\n",
    "    \n",
    "    def check_win(self, state, action):\n",
    "        if action == None:\n",
    "            return False\n",
    "        \n",
    "        row = action // self.column_count\n",
    "        column = action % self.column_count\n",
    "        player = state[row, column]\n",
    "        \n",
    "        return (\n",
    "            np.sum(state[row, :]) == player * self.column_count\n",
    "            or np.sum(state[:, column]) == player * self.row_count\n",
    "            or np.sum(np.diag(state)) == player * self.row_count\n",
    "            or np.sum(np.diag(np.flip(state, axis=0))) == player * self.row_count\n",
    "        )\n",
    "    \n",
    "    def get_value_and_terminated(self, state, action):\n",
    "        if self.check_win(state, action):\n",
    "            return 1, True\n",
    "        if np.sum(self.get_valid_moves(state)) == 0:\n",
    "            return 0, True\n",
    "        return 0, False\n",
    "    \n",
    "    def get_opponent(self, player):\n",
    "        return -player\n",
    "    \n",
    "    def get_opponent_value(self, value):\n",
    "        return -value\n",
    "    \n",
    "    def change_perspective(self, state, player):\n",
    "        return state * player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, args, game):\n",
    "        self.memory = []\n",
    "        self.trajectories = []\n",
    "        self.args = args\n",
    "        self.game = game\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "    def empty(self):\n",
    "        self.memory = []\n",
    "        self.trajectories = []\n",
    "\n",
    "    def build_trajectories(self):\n",
    "        for i in range(len(self.memory)):\n",
    "            observation, action, policy, value, game_idx, is_terminal = self.memory[i]\n",
    "            if is_terminal:\n",
    "                action = np.random.choice(self.game.action_size)\n",
    "\n",
    "            policy_list, action_list, value_list = [policy], [action], [value]\n",
    "\n",
    "            for k in range(1, self.args['K'] + 1):\n",
    "                if i + k < len(self.memory) and self.memory[i + k][4] == game_idx:\n",
    "                    _, action, policy, value, _, is_terminal = self.memory[i + k]\n",
    "                    if is_terminal:\n",
    "                        action = np.random.choice(self.game.action_size)\n",
    "                    policy_list.append(policy)\n",
    "                    action_list.append(action)\n",
    "                    value_list.append(value)\n",
    "\n",
    "                else:\n",
    "                    # might be wrong! Check paper\n",
    "                    policy_list.append(policy_list[-1])\n",
    "                    action_list.append(np.random.choice(self.game.action_size))\n",
    "                    value_list.append(0)\n",
    "\n",
    "            policy_list = np.stack(policy_list)\n",
    "            self.trajectories.append((observation, policy_list, action_list, value_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, muZero, game, args, state, parent=None, action_taken=None, prior=0, visit_count=0):\n",
    "        self.muZero = muZero\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.action_taken = action_taken\n",
    "        self.prior = prior\n",
    "        self.children = []\n",
    "        \n",
    "        self.visit_count = visit_count\n",
    "        self.value_sum = 0\n",
    "        \n",
    "    def is_fully_expanded(self):\n",
    "        return len(self.children) > 0\n",
    "    \n",
    "    def select(self):\n",
    "        best_child = None\n",
    "        best_ucb = -np.inf\n",
    "        \n",
    "        for child in self.children:\n",
    "            ucb = self.get_ucb(child)\n",
    "            if ucb > best_ucb:\n",
    "                best_child = child\n",
    "                best_ucb = ucb\n",
    "                \n",
    "        return best_child\n",
    "    \n",
    "    def get_ucb(self, child):\n",
    "        if child.visit_count == 0:\n",
    "            q_value = 0\n",
    "        else:\n",
    "            q_value = 1 - ((child.value_sum / child.visit_count) + 1) / 2\n",
    "        return q_value + self.args['C'] * (math.sqrt(self.visit_count) / (child.visit_count + 1)) * child.prior\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def expand(self, policy):\n",
    "        child_state = self.state.copy()\n",
    "        child_state = np.expand_dims(child_state, axis=0).repeat(len(policy), axis=0)\n",
    "\n",
    "        child_state = self.muZero.dynamics(\n",
    "            torch.tensor(child_state, dtype=torch.float32, device=self.muZero.device), list(range(len(policy))))\n",
    "        child_state = child_state.cpu().numpy()\n",
    "        child_state = self.game.change_perspective(child_state, player=self.game.get_opponent(1))\n",
    "        \n",
    "        for action, prob in enumerate(policy):\n",
    "            child = Node(\n",
    "                self.muZero,\n",
    "                self.game,\n",
    "                self.args,\n",
    "                child_state[action],\n",
    "                self,\n",
    "                action,\n",
    "                prob\n",
    "            )\n",
    "            self.children.append(child)\n",
    "\n",
    "    def backpropagate(self, value):\n",
    "        self.value_sum += value\n",
    "        self.visit_count += 1\n",
    "\n",
    "        if self.parent is not None:\n",
    "            value = self.game.get_opponent_value(value)\n",
    "            self.parent.backpropagate(value)\n",
    "\n",
    "class MCTS:\n",
    "    def __init__(self, muZero, game, args):\n",
    "        self.muZero = muZero\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def search(self, state):\n",
    "        hidden_state = self.muZero.represent(\n",
    "            torch.tensor(state, dtype=torch.float32, device=self.muZero.device).unsqueeze(0)\n",
    "        )\n",
    "        policy, _ = self.muZero.predict(hidden_state)\n",
    "        hidden_state = hidden_state.cpu().numpy().squeeze(0)\n",
    "        \n",
    "        root = Node(self.muZero, self.game, self.args, hidden_state, visit_count=1)\n",
    "\n",
    "        policy = torch.softmax(policy, dim=1).squeeze(0).cpu().numpy()\n",
    "        policy = (1 - self.args['dirichlet_epsilon']) * policy + self.args['dirichlet_epsilon'] \\\n",
    "            * np.random.dirichlet([self.args['dirichlet_alpha']] * self.game.action_size)\n",
    "        policy /= np.sum(policy)\n",
    "\n",
    "        root.expand(policy)\n",
    "\n",
    "        for search in range(self.args['num_mcts_searches']):\n",
    "            node = root\n",
    "\n",
    "            while node.is_fully_expanded():\n",
    "                node = node.select()\n",
    "\n",
    "            policy, value = self.muZero.predict(\n",
    "                torch.tensor(node.state, dtype=torch.float32, device=self.muZero.device).unsqueeze(0)\n",
    "            )\n",
    "            policy = torch.softmax(policy, dim=1).squeeze().cpu().numpy()\n",
    "            value = value.item()\n",
    "\n",
    "            node.expand(policy)\n",
    "            node.backpropagate(value)\n",
    "\n",
    "        action_probs = np.zeros(self.game.action_size)\n",
    "        for child in root.children:\n",
    "            action_probs[child.action_taken] = child.visit_count\n",
    "        action_probs /= np.sum(action_probs)\n",
    "        return action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MuZero(nn.Module):\n",
    "    def __init__(self, game, device):\n",
    "        super().__init__()\n",
    "        self.game = game\n",
    "        self.device = device\n",
    "\n",
    "        self.predictionFunction = PredictionFunction(game)\n",
    "        self.dynamicsFunction = DynamicsFunction(game)\n",
    "        self.representationFunction = RepresentationFunction(game)\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def predict(self, hidden_state):\n",
    "        return self.predictionFunction(hidden_state)\n",
    "\n",
    "    def represent(self, observation):\n",
    "        return self.representationFunction(observation)\n",
    "\n",
    "    def dynamics(self, hidden_state, actions):\n",
    "        actionPlane = torch.zeros((hidden_state.shape[0], self.game.action_size), device=self.device, dtype=torch.float32)\n",
    "        for i, a in enumerate(actions):\n",
    "            actionPlane[i, a] = 1\n",
    "        x = torch.cat((hidden_state, actionPlane), dim=1)\n",
    "        return self.dynamicsFunction(x)\n",
    "\n",
    "class DynamicsFunction(nn.Module):\n",
    "    def __init__(self, game):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(32 + game.action_size, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "    \n",
    "class PredictionFunction(nn.Module):\n",
    "    def __init__(self, game):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.policy_head = nn.Sequential(\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, game.action_size)\n",
    "        )\n",
    "\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        p = self.policy_head(x)\n",
    "        v = self.value_head(x)\n",
    "        return p, v\n",
    "\n",
    "# Creates initial hidden state based on observation | several observations\n",
    "class RepresentationFunction(nn.Module):\n",
    "    def __init__(self, game):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(game.row_count * game.column_count, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, muZero, optimizer, game, args):\n",
    "        self.muZero = muZero\n",
    "        self.optimizer = optimizer\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.mcts = MCTS(self.muZero, self.game, self.args)\n",
    "        self.replayBuffer = ReplayBuffer(self.args, self.game)\n",
    "\n",
    "    def self_play(self, game_idx):\n",
    "        memory = []\n",
    "        player = 1\n",
    "        observation = self.game.get_initial_state()\n",
    "\n",
    "        while True:\n",
    "            neutral_observation = self.game.change_perspective(observation, player)\n",
    "            action_probs = self.mcts.search(neutral_observation)\n",
    "\n",
    "            temperature_action_probs = action_probs ** (1 / self.args['temperature'])\n",
    "            temperature_action_probs /= np.sum(temperature_action_probs)\n",
    "            action = np.random.choice(self.game.action_size, p=temperature_action_probs)\n",
    "\n",
    "            memory.append((neutral_observation, action, action_probs, player))\n",
    "\n",
    "            observation = self.game.get_next_state(observation, action, player)\n",
    "\n",
    "            value, is_terminal = self.game.get_value_and_terminated(observation, action)\n",
    "\n",
    "            if is_terminal:\n",
    "                returnMemory = []\n",
    "                for hist_neutral_observation, hist_action, hist_action_probs, hist_player in memory:\n",
    "                    hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n",
    "                    returnMemory.append((\n",
    "                        hist_neutral_observation,\n",
    "                        hist_action, \n",
    "                        hist_action_probs,\n",
    "                        hist_outcome,\n",
    "                        game_idx,\n",
    "                        False # is_terminal\n",
    "                    ))\n",
    "                if self.args['K'] > 0:\n",
    "                    returnMemory.append((\n",
    "                        self.game.change_perspective(observation, self.game.get_opponent(player)), # check this\n",
    "                        None,\n",
    "                        np.array([1 / self.game.action_size] * self.game.action_size),\n",
    "                        0, # check this\n",
    "                        game_idx,\n",
    "                        True # is_terminal\n",
    "                    ))\n",
    "                return returnMemory\n",
    "            \n",
    "            player = self.game.get_opponent(player)\n",
    "\n",
    "    def train(self):\n",
    "        random.shuffle(self.replayBuffer.trajectories)\n",
    "        for batchIdx in range(0, len(self.replayBuffer), self.args['batch_size']): \n",
    "            sample = self.replayBuffer. trajectories[batchIdx:min(len(self.replayBuffer) - 1, batchIdx + self.args['batch_size'])]\n",
    "            observation, policy_targets, action, value_targets = list(zip(*sample))\n",
    "\n",
    "            observation = torch.tensor(np.array(observation), dtype=torch.float32, device=self.muZero.device)\n",
    "            action = np.array(action)\n",
    "            policy_targets = torch.tensor(np.array(policy_targets), dtype=torch.float32, device=self.muZero.device)\n",
    "            value_targets = torch.tensor(np.array(value_targets), dtype=torch.float32, device=self.muZero.device).unsqueeze(-1)\n",
    "\n",
    "            hidden_state = self.muZero.represent(observation)\n",
    "            out_policy, out_value = self.muZero.predict(hidden_state)\n",
    "\n",
    "            policy_loss = F.cross_entropy(out_policy, policy_targets[:, 0]) \n",
    "            value_loss = F.mse_loss(out_value, value_targets[:, 0])\n",
    "\n",
    "            if self.args['K'] > 0:\n",
    "                for k in range(1, self.args['K'] + 1):\n",
    "                    hidden_state, self.muZero.dynamics(hidden_state, action[:, k - 1])\n",
    "                    hidden_state.register_hook(lambda grad: grad * 0.5)\n",
    "\n",
    "                    hidden_state = self.game.change_perspective(hidden_state, -1)\n",
    "\n",
    "                    out_policy, out_value = self.muZero.predict(hidden_state)\n",
    "\n",
    "                    policy_loss += F.cross_entropy(out_policy, policy_targets[:, k])\n",
    "                    value_loss += F.mse_loss(out_value, value_targets[:, k])\n",
    "\n",
    "            loss = (value_loss * self.args['value_loss_weight'] + policy_loss).mean()\n",
    "            loss.register_hook(lambda grad: grad * 1 / self.args['K'])\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.muZero.parameters(), self.args['max_grad_norm'])\n",
    "            self.optimizer.step()\n",
    "\n",
    "    def run(self):\n",
    "        for iteration in range(self.args['num_iterations']):\n",
    "            print(f\"iteration: {iteration}\")\n",
    "            self.replayBuffer.empty()\n",
    "\n",
    "            self.muZero.eval()\n",
    "            for train_game_idx in (self_play_bar := trange(self.args['num_train_games'], desc=\"train_game\")):\n",
    "                self.replayBuffer.memory += self.self_play(train_game_idx + iteration * self.args['num_train_games'])\n",
    "                self_play_bar.set_description(f\"Avg. steps per Game: {len(self.replayBuffer) / (train_game_idx + 1):.2f}\")\n",
    "            self.replayBuffer.build_trajectories()\n",
    "\n",
    "            self.muZero.train()\n",
    "            for epoch in trange(self.args['num_epochs'], desc=\"epochs\"):\n",
    "                self.train()\n",
    "\n",
    "            torch.save(self.muZero.state_dict(), f\"../../Environments/{self.game}/Models/model_{iteration}.pt\")\n",
    "            torch.save(self.optimizer.state_dict(), f\"../../Environments/{self.game}/Models/optimizer_{iteration}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f27aeb4ab8ff408e829edbc2a889ea86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_game:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d52241b65628431cb3ea87bad6bedba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epochs:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49ccdf687c464b9a9c1fa514fedcbed8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_game:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f239c3b4a9e04d2bb302abc7a619a957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epochs:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fda313e5270d4304b51c2048d8e74802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_game:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8617e4c4a09471a9eedf9137afea7a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epochs:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61a176516dd24dbfb0246dda5895a741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_game:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a419f3cd6324151bebeb85ca42b607b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epochs:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6678927f17004ee4aedbaeb8a5f2e1a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_game:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "708aad380230432d9a3934e47d3c49a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epochs:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3e08ab8b3a24405af4490a79e52b762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_game:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "345e31d38e6d477a86d398a4da246eba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epochs:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8869738d9e343e7bd29dec53e2bfd95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_game:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e5bc7d5075f42238e33199be32aa6cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epochs:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fd0d70b6d5f4accb198a312f45a2a98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_game:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20e9ea6f32c74c1b8262d4a34dd97c2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epochs:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d7f2ee0820a4bb2ad6cd9e5e6c406f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_game:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1a3b8f8a35345fdbb1b2dc773239c6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epochs:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b51d546df5f54da5849e21a71a524606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_game:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97521710fa3a4c6dbccc8b1cc4df252b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epochs:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0434b18b377c422c80e043ab13b4a086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_game:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18ea894237844973b6eb86d862f1d370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epochs:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f705fb9c61474b1197f575cfdcd7cc8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_game:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75b5af9b213f46f4b34e18cd6de11abf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epochs:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0880847609c44a2eab518b9b4f304526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_game:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9127c461eeb4859978429d914273d91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epochs:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "014ac0e3a0ef4a24a3ebd620e707856a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_game:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "988a16d59ddb4234ab4464ee5cef720a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epochs:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f605308d35d4c6da20acf73bd57833b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_game:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81b1fa95b93943d48d99c61d96abf4ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epochs:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bca78a05df19420eb7f21a04a6771ef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_game:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00a4556cd7f54ca8a8e04327d58e3cf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epochs:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f09bcca440c44d2b785150b9458e56a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_game:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a2ed53d737a4c7080dc8f9d3c73855e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epochs:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 17\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c1f35f664b84f17aaf7f75d15df88d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_game:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55e8c8279df347ceb6dfbae4d90ccf3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epochs:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3863b288bfc14447aaf6ac755402e656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_game:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c330f82e63434bdb89125feb0ce9468c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epochs:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc9047771f8446a7adcba600a4e346f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_game:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8a06b498e43484196c2da9e8f821cf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epochs:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "args = {\n",
    "    'num_iterations': 20,\n",
    "    'num_train_games': 100,\n",
    "    'num_mcts_searches': 50,\n",
    "    'num_epochs': 4,\n",
    "    'batch_size': 64,\n",
    "    'temperature': 1,\n",
    "    'K': 3,\n",
    "    'C': 2,\n",
    "    'dirichlet_alpha': 0.3,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'value_loss_weight': 0.25,\n",
    "    'max_grad_norm': 5,\n",
    "}\n",
    "\n",
    "LOAD = False\n",
    "\n",
    "game = TicTacToe()\n",
    "muZero = MuZero(game, device)\n",
    "optimizer = torch.optim.AdamW(muZero.parameters(), lr=0.001)\n",
    "\n",
    "trainer = Trainer(muZero, optimizer, game, args)\n",
    "trainer.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2177f1ca12c1330a133c1d40b46100b268ab447cddcbdfdc0c7b2b7e4840e700"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
