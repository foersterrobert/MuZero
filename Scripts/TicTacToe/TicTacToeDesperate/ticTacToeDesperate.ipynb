{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm.notebook import trange\n",
    "import wandb\n",
    "\n",
    "from game import TicTacToe\n",
    "from resNet import MuZeroResNet as MuZero\n",
    "# from linearNet import MuZeroLinear as MuZero\n",
    "from mcts import MCTS\n",
    "from replayBuffer import ReplayBuffer\n",
    "from utils import KaggleAgent, evaluateKaggle\n",
    "\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "wandb.login()\n",
    "wandb.init(project=\"TicTacToe-Desperate\")\n",
    "\n",
    "# no masking for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, muZero, optimizer, game, args):\n",
    "        self.muZero = muZero\n",
    "        self.optimizer = optimizer\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.mcts = MCTS(self.muZero, self.game, self.args)\n",
    "        self.replayBuffer = ReplayBuffer(self.args, self.game)\n",
    "        if self.args['evaluate']:\n",
    "            self.evalPlayer = KaggleAgent(self.muZero, self.game, self.args['eval_args'])\n",
    "\n",
    "    def self_play(self, game_idx):\n",
    "        memory = []\n",
    "        player = 1\n",
    "        observation = self.game.get_initial_state()\n",
    "\n",
    "        while True:\n",
    "            valid_moves = self.game.get_valid_moves(observation)\n",
    "            neutral_observation = self.game.change_perspective(observation, player)\n",
    "            encoded_observation = self.game.get_encoded_observation(neutral_observation)\n",
    "            action_probs = self.mcts.search(encoded_observation, valid_moves)\n",
    "\n",
    "            temperature_action_probs = action_probs ** (1 / self.args['temperature'])\n",
    "            temperature_action_probs /= np.sum(temperature_action_probs)\n",
    "            action = np.random.choice(self.game.action_size, p=temperature_action_probs)\n",
    "\n",
    "            memory.append((encoded_observation, action, action_probs, player))\n",
    "\n",
    "            observation = self.game.get_next_state(observation, action, player)\n",
    "            value, is_terminal = self.game.get_value_and_terminated(observation, action)\n",
    "\n",
    "            if is_terminal:\n",
    "                return_memory = []\n",
    "                for hist_observation, hist_action, hist_action_probs, hist_player in memory:\n",
    "                    hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n",
    "                    return_memory.append((\n",
    "                        hist_observation,\n",
    "                        hist_action, \n",
    "                        hist_action_probs,\n",
    "                        hist_outcome,\n",
    "                        game_idx,\n",
    "                        False # is_terminal\n",
    "                    ))\n",
    "                hist_outcome = value if self.game.get_opponent(player) == player else self.game.get_opponent_value(value)\n",
    "                return_memory.append((\n",
    "                    self.game.get_encoded_observation(self.game.change_perspective(observation, self.game.get_opponent(player))),\n",
    "                    None,\n",
    "                    np.ones(self.game.action_size) / self.game.action_size,\n",
    "                    hist_outcome,\n",
    "                    game_idx,\n",
    "                    True # is_terminal\n",
    "                ))\n",
    "                return return_memory\n",
    "            \n",
    "            player = self.game.get_opponent(player)\n",
    "\n",
    "    def train(self):\n",
    "        random.shuffle(self.replayBuffer.trajectories)\n",
    "        for batchIdx in range(0, len(self.replayBuffer), self.args['batch_size']): \n",
    "            sample = self.replayBuffer.trajectories[batchIdx:batchIdx+self.args['batch_size']]\n",
    "            observation, policy_targets, action, value_targets = list(zip(*sample))\n",
    "\n",
    "            observation = torch.tensor(np.array(observation), dtype=torch.float32, device=self.muZero.device)\n",
    "            action = np.array(action)\n",
    "            policy_targets = torch.tensor(np.array(policy_targets), dtype=torch.float32, device=self.muZero.device)\n",
    "            value_targets = torch.tensor(np.array(value_targets), dtype=torch.float32, device=self.muZero.device).unsqueeze(-1)\n",
    "\n",
    "            hidden_state = self.muZero.represent(observation)\n",
    "            out_policy, out_value = self.muZero.predict(hidden_state)\n",
    "            \n",
    "            policy_loss = F.cross_entropy(out_policy, policy_targets[:, 0])\n",
    "            value_loss = F.mse_loss(out_value, value_targets[:, 0])\n",
    "            for k in range(1, self.args['K'] + 1):\n",
    "                hidden_state, self.muZero.dynamics(hidden_state, action[:, k - 1])\n",
    "                out_policy, out_value = self.muZero.predict(hidden_state)\n",
    "\n",
    "                current_policy_loss = F.cross_entropy(out_policy, policy_targets[:, k])\n",
    "                current_value_loss = F.mse_loss(out_value, value_targets[:, k])\n",
    "\n",
    "                current_policy_loss.register_hook(lambda grad: grad / self.args['K'])\n",
    "                current_value_loss.register_hook(lambda grad: grad / self.args['K'])\n",
    "\n",
    "                policy_loss += current_policy_loss\n",
    "                value_loss += current_value_loss\n",
    "\n",
    "                # hidden_state.register_hook(lambda grad: grad * 0.5)\n",
    "\n",
    "            loss = value_loss * self.args['value_loss_weight'] + policy_loss\n",
    "\n",
    "            wandb.log({\n",
    "                \"value_loss\": value_loss.item(),\n",
    "                \"policy_loss\": policy_loss.item(),\n",
    "                \"loss\": loss.item()\n",
    "            })\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.muZero.parameters(), self.args['max_grad_norm'])\n",
    "            self.optimizer.step()\n",
    "\n",
    "    def evaluate(self):\n",
    "        results_1 = evaluateKaggle(\"tictactoe\", [\"random\", self.evalPlayer], num_iterations=20)\n",
    "        results_2 = evaluateKaggle(\"tictactoe\", [self.evalPlayer, \"random\"], num_iterations=20)\n",
    "        \n",
    "        wandb.log({\n",
    "            \"win_rate (against random)\": (np.sum(results_1==-1) + np.sum(results_2==1)) / 40,\n",
    "            \"tie_rate (against random)\": (np.sum(results_1==0) + np.sum(results_2==0)) / 40,\n",
    "        })\n",
    "\n",
    "    def run(self):\n",
    "        for iteration in range(self.args['num_iterations']):\n",
    "            print(f\"iteration: {iteration}\")\n",
    "            self.replayBuffer.empty()\n",
    "\n",
    "            self.muZero.eval()\n",
    "            for train_game_idx in (self_play_bar := trange(self.args['num_train_games'], desc=\"train_game\")):\n",
    "                self.replayBuffer.memory += self.self_play(train_game_idx + iteration * self.args['num_train_games'])\n",
    "                self_play_bar.set_description(f\"Avg. steps per Game: {len(self.replayBuffer.memory) / (train_game_idx + 1):.2f}\")\n",
    "                wandb.log({\"steps_per_game\": len(self.replayBuffer.memory) / (train_game_idx + 1)})\n",
    "            self.replayBuffer.build_trajectories()\n",
    "\n",
    "            self.muZero.train()\n",
    "            for epoch in trange(self.args['num_epochs'], desc=\"epochs\"):\n",
    "                self.train()\n",
    "\n",
    "            if self.args['evaluate']:\n",
    "                self.muZero.eval()\n",
    "                self.evaluate()\n",
    "\n",
    "            torch.save(self.muZero.state_dict(), f\"Models/model_{iteration}.pt\")\n",
    "            torch.save(self.optimizer.state_dict(), f\"Models/optimizer_{iteration}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "args = {\n",
    "    'num_iterations': 20,\n",
    "    'num_train_games': 500,\n",
    "    'num_mcts_searches': 25,\n",
    "    'num_epochs': 4,\n",
    "    'batch_size': 64,\n",
    "    'temperature': 1,\n",
    "    'K': 3,\n",
    "    'C': 2,\n",
    "    'dirichlet_alpha': 0.1,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'value_loss_weight': 0.25,\n",
    "    'max_grad_norm': 5,\n",
    "    'evaluate': True,\n",
    "    'eval_args': {\n",
    "        'search': True,\n",
    "        'num_mcts_searches': 25,\n",
    "        'temperature': 0.1,\n",
    "        'C': 2,\n",
    "        'dirichlet_alpha': 0.3,\n",
    "        'dirichlet_epsilon': 0.25,\n",
    "        'num_eval_games': 100,\n",
    "    }\n",
    "}\n",
    "\n",
    "LOAD = False\n",
    "\n",
    "game = TicTacToe()\n",
    "muZero = MuZero(game, device)\n",
    "optimizer = torch.optim.Adam(muZero.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "if LOAD:\n",
    "    muZero.load_state_dict(torch.load(f\"Models/model.pt\"))\n",
    "    optimizer.load_state_dict(torch.load(f\"Models/optimizer.pt\"))\n",
    "\n",
    "trainer = Trainer(muZero, optimizer, game, args)\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "from utils import KaggleAgent, evaluateKaggle\n",
    "\n",
    "args = {\n",
    "    'num_mcts_searches': 25,\n",
    "    'temperature': 1,\n",
    "    'C': 1.25,\n",
    "    'dirichlet_alpha': 0.1,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'search': True,\n",
    "}\n",
    "\n",
    "game = TicTacToe()\n",
    "muZero = MuZero(game, device)\n",
    "\n",
    "# muZero.load_state_dict(torch.load(\"../../Environments/TicTacToe/Models/model_15.pt\"))\n",
    "muZero.eval()\n",
    "\n",
    "player = KaggleAgent(muZero, game, args)\n",
    "\n",
    "evaluateKaggle(\"tictactoe\", [\"random\", player.run], num_iterations=1)\n",
    "evaluateKaggle(\"tictactoe\", [player.run, \"random\"], num_iterations=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Parameters\n",
    "game = TicTacToe()\n",
    "model = MuZero(game, device)\n",
    "\n",
    "def get_n_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp\n",
    "\n",
    "get_n_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mresNetGeneral\u001b[39;00m \u001b[39mimport\u001b[39;00m MuZeroResidualNetwork\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtictactoeGeneral\u001b[39;00m \u001b[39mimport\u001b[39;00m TicTacToe \u001b[39mas\u001b[39;00m TicTacToeGeneral\n\u001b[0;32m----> 8\u001b[0m model \u001b[39m=\u001b[39m MuZeroResidualNetwork(\n\u001b[1;32m      9\u001b[0m     observation_shape\u001b[39m=\u001b[39;49m(\u001b[39m3\u001b[39;49m, \u001b[39m3\u001b[39;49m, \u001b[39m3\u001b[39;49m),\n\u001b[1;32m     10\u001b[0m     stacked_observations\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m     11\u001b[0m     action_space_size\u001b[39m=\u001b[39;49m\u001b[39mlist\u001b[39;49m(\u001b[39mrange\u001b[39;49m(\u001b[39m9\u001b[39;49m)),\n\u001b[1;32m     12\u001b[0m     num_blocks\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     13\u001b[0m     num_channels\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m,\n\u001b[1;32m     14\u001b[0m     reduced_channels_reward\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m,\n\u001b[1;32m     15\u001b[0m     reduced_channels_value\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m,\n\u001b[1;32m     16\u001b[0m     reduced_channels_policy\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m,\n\u001b[1;32m     17\u001b[0m     fc_reward_layers\u001b[39m=\u001b[39;49m[\u001b[39m8\u001b[39;49m],\n\u001b[1;32m     18\u001b[0m     fc_value_layers\u001b[39m=\u001b[39;49m[\u001b[39m8\u001b[39;49m],\n\u001b[1;32m     19\u001b[0m     fc_policy_layers\u001b[39m=\u001b[39;49m[\u001b[39m8\u001b[39;49m],\n\u001b[1;32m     20\u001b[0m     support_size\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[1;32m     21\u001b[0m     downsample\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     24\u001b[0m a \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39m/home/robert/Documents/GitHub/MuZero/MuZeroGeneral/2023-05-25--14-22-49/model.checkpoint\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     25\u001b[0m a\n",
      "File \u001b[0;32m~/Documents/GitHub/MuZero/Scripts/TicTacToe/TicTacToeDesperate/resNetGeneral.py:76\u001b[0m, in \u001b[0;36mMuZeroResidualNetwork.__init__\u001b[0;34m(self, observation_shape, stacked_observations, action_space_size, num_blocks, num_channels, reduced_channels_reward, reduced_channels_value, reduced_channels_policy, fc_reward_layers, fc_value_layers, fc_policy_layers, support_size, downsample)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrepresentation_network \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mDataParallel(\n\u001b[1;32m     55\u001b[0m     RepresentationNetwork(\n\u001b[1;32m     56\u001b[0m         observation_shape,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m     )\n\u001b[1;32m     62\u001b[0m )\n\u001b[1;32m     64\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdynamics_network \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mDataParallel(\n\u001b[1;32m     65\u001b[0m     DynamicsNetwork(\n\u001b[1;32m     66\u001b[0m         num_blocks,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     72\u001b[0m     )\n\u001b[1;32m     73\u001b[0m )\n\u001b[1;32m     75\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_network \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mDataParallel(\n\u001b[0;32m---> 76\u001b[0m     PredictionNetwork(\n\u001b[1;32m     77\u001b[0m         action_space_size,\n\u001b[1;32m     78\u001b[0m         num_blocks,\n\u001b[1;32m     79\u001b[0m         num_channels,\n\u001b[1;32m     80\u001b[0m         reduced_channels_value,\n\u001b[1;32m     81\u001b[0m         reduced_channels_policy,\n\u001b[1;32m     82\u001b[0m         fc_value_layers,\n\u001b[1;32m     83\u001b[0m         fc_policy_layers,\n\u001b[1;32m     84\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfull_support_size,\n\u001b[1;32m     85\u001b[0m         block_output_size_value,\n\u001b[1;32m     86\u001b[0m         block_output_size_policy,\n\u001b[1;32m     87\u001b[0m     )\n\u001b[1;32m     88\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/GitHub/MuZero/Scripts/TicTacToe/TicTacToeDesperate/resNetGeneral.py:380\u001b[0m, in \u001b[0;36mPredictionNetwork.__init__\u001b[0;34m(self, action_space_size, num_blocks, num_channels, reduced_channels_value, reduced_channels_policy, fc_value_layers, fc_policy_layers, full_support_size, block_output_size_value, block_output_size_policy)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblock_output_size_policy \u001b[39m=\u001b[39m block_output_size_policy\n\u001b[1;32m    377\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc_value \u001b[39m=\u001b[39m mlp(\n\u001b[1;32m    378\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblock_output_size_value, fc_value_layers, full_support_size\n\u001b[1;32m    379\u001b[0m )\n\u001b[0;32m--> 380\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc_policy \u001b[39m=\u001b[39m mlp(\n\u001b[1;32m    381\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblock_output_size_policy,\n\u001b[1;32m    382\u001b[0m     fc_policy_layers,\n\u001b[1;32m    383\u001b[0m     action_space_size,\n\u001b[1;32m    384\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/GitHub/MuZero/Scripts/TicTacToe/TicTacToeDesperate/resNetGeneral.py:397\u001b[0m, in \u001b[0;36mmlp\u001b[0;34m(input_size, layer_sizes, output_size, output_activation, activation)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(sizes) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m    396\u001b[0m     act \u001b[39m=\u001b[39m activation \u001b[39mif\u001b[39;00m i \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(sizes) \u001b[39m-\u001b[39m \u001b[39m2\u001b[39m \u001b[39melse\u001b[39;00m output_activation\n\u001b[0;32m--> 397\u001b[0m     layers \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mLinear(sizes[i], sizes[i \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m]), act()]\n\u001b[1;32m    398\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mSequential(\u001b[39m*\u001b[39mlayers)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/torch/nn/modules/linear.py:96\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_features \u001b[39m=\u001b[39m in_features\n\u001b[1;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_features \u001b[39m=\u001b[39m out_features\n\u001b[0;32m---> 96\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39;49mempty((out_features, in_features), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfactory_kwargs))\n\u001b[1;32m     97\u001b[0m \u001b[39mif\u001b[39;00m bias:\n\u001b[1;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39mempty(out_features, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs))\n",
      "\u001b[0;31mTypeError\u001b[0m: empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "# Test MuZeroGeneral\n",
    "\n",
    "from resNetGeneral import MuZeroResidualNetwork\n",
    "from tictactoeGeneral import TicTacToe as TicTacToeGeneral\n",
    "\n",
    "\n",
    "\n",
    "model = MuZeroResidualNetwork(\n",
    "    observation_shape=(3, 3, 3),\n",
    "    stacked_observations=0,\n",
    "    action_space_size=list(range(9)),\n",
    "    num_blocks=1,\n",
    "    num_channels=16,\n",
    "    reduced_channels_reward=16,\n",
    "    reduced_channels_value=16,\n",
    "    reduced_channels_policy=16,\n",
    "    fc_reward_layers=[8],\n",
    "    fc_value_layers=[8],\n",
    "    fc_policy_layers=[8],\n",
    "    support_size=10,\n",
    "    downsample=False,\n",
    ")\n",
    "\n",
    "a = torch.load('/home/robert/Documents/GitHub/MuZero/MuZeroGeneral/2023-05-25--14-22-49/model.checkpoint')\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2177f1ca12c1330a133c1d40b46100b268ab447cddcbdfdc0c7b2b7e4840e700"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
