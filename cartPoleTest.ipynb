{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Prioritized Experience Replay\n",
    "# Make model much smaller\n",
    "# Reward missing in UCB\n",
    "# Look at past obs and actions when generating hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPole:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make('CartPole-v1')\n",
    "        self.action_size = self.env.action_space.n\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'CartPole-v1'\n",
    "\n",
    "    def get_initial_state(self):\n",
    "        observation, info = self.env.reset()\n",
    "        valid_locations = self.get_valid_locations(observation)\n",
    "        reward = 0\n",
    "        is_terminal = False\n",
    "        return observation, valid_locations, reward, is_terminal\n",
    "\n",
    "    def step(self, action):\n",
    "        observation, reward, is_terminal, _, _ = self.env.step(action)\n",
    "        valid_locations = self.get_valid_locations(observation)\n",
    "        return observation, valid_locations, reward, is_terminal\n",
    "\n",
    "    def get_valid_locations(self, observation):\n",
    "        return self.action_size\n",
    "\n",
    "    def get_canonical_state(self, hidden_state, player):\n",
    "        return hidden_state\n",
    "\n",
    "    def get_encoded_observation(self, observation):\n",
    "        encoded_observation = observation.copy()\n",
    "        encoded_observation[0] = (encoded_observation[0] + 4.8) / 9.6\n",
    "        encoded_observation[2] = (encoded_observation[2] + 0.42) / 0.84\n",
    "        return encoded_observation\n",
    "\n",
    "    def get_opponent_player(self, player):\n",
    "        return player\n",
    "\n",
    "    def get_opponent_value(self, value):\n",
    "        return value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, args, game):\n",
    "        self.memory = []\n",
    "        self.trajectories = []\n",
    "        self.args = args\n",
    "        self.game = game\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    def empty(self):\n",
    "        self.memory = []\n",
    "\n",
    "    def build_trajectories(self):\n",
    "        self.trajectories = []\n",
    "        for i in range(len(self.memory)):\n",
    "            observation, action, policy, reward, game_idx, is_terminal = self.memory[i]\n",
    "            if is_terminal:\n",
    "                action = np.random.choice(self.game.action_size)\n",
    "\n",
    "            policy_list, action_list, value_list, reward_list = [policy], [action], [], [reward]\n",
    "\n",
    "            current_value = []\n",
    "            for n in range(1, self.args['N'] + 1):\n",
    "                if i + n < len(self.memory) and self.memory[i + n][4] == game_idx:\n",
    "                    _, _, _, reward, _, _ = self.memory[i + n]\n",
    "                    current_value.append(reward)\n",
    "                else:\n",
    "                    current_value.append(0)\n",
    "            value = np.sum([self.args['gamma'] ** n * current_value[n - 1] for n in range(1, self.args['N'] + 1)])\n",
    "            value = (value - np.sum(\n",
    "                        [self.args['gamma'] ** n * 1 for n in range(1, self.args['N'] + 1)]\n",
    "                    ) * 0.5) / (np.sum(\n",
    "                        [self.args['gamma'] ** n * 1 for n in range(1, self.args['N'] + 1)]\n",
    "                    ))\n",
    "            value_list.append(value)\n",
    "\n",
    "            for k in range(1, self.args['K'] + 1):\n",
    "                if i + k < len(self.memory) and self.memory[i + k][4] == game_idx:\n",
    "                    _, action, policy, reward, _, is_terminal = self.memory[i + k]\n",
    "                    if is_terminal:\n",
    "                        action = np.random.choice(self.game.action_size)\n",
    "                    action_list.append(action)\n",
    "                    policy_list.append(policy)\n",
    "                    reward_list.append(reward)\n",
    "\n",
    "                    current_value = []\n",
    "                    for n in range(1, self.args['N'] + 1):\n",
    "                        if i + k + n < len(self.memory) and self.memory[i + k + n][4] == game_idx:\n",
    "                            _, _, _, reward, _, _ = self.memory[i + k + n]\n",
    "                            current_value.append(reward)\n",
    "                        else:\n",
    "                            current_value.append(0)\n",
    "                    value = np.sum([self.args['gamma'] ** n * current_value[n - 1] for n in range(1, self.args['N'] + 1)])\n",
    "                    value = (value - np.sum(\n",
    "                        [self.args['gamma'] ** n * 1 for n in range(1, self.args['N'] + 1)]\n",
    "                    ) * 0.5) / (np.sum(\n",
    "                        [self.args['gamma'] ** n * 1 for n in range(1, self.args['N'] + 1)]\n",
    "                    ))\n",
    "\n",
    "                    value_list.append(value)\n",
    "\n",
    "                else:\n",
    "                    action_list.append(np.random.choice(self.game.action_size))\n",
    "                    policy_list.append(policy_list[-1])\n",
    "                    value_list.append(0)\n",
    "                    reward_list.append(0)\n",
    "\n",
    "            policy_list = np.stack(policy_list)\n",
    "            self.trajectories.append((observation, action_list, policy_list, value_list, reward_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, state, reward, prior, muZero, args, game, parent=None, action_taken=None):\n",
    "        self.state = state\n",
    "        self.reward = reward\n",
    "        self.children = []\n",
    "        self.parent = parent\n",
    "        self.total_value = 0\n",
    "        self.visit_count = 0\n",
    "        self.prior = prior\n",
    "        self.muZero = muZero\n",
    "        self.action_taken = action_taken\n",
    "        self.args = args\n",
    "        self.game = game\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def expand(self, action_probs):\n",
    "        actions = [a for a in range(self.game.action_size) if action_probs[a] > 0]\n",
    "        expand_state = self.state.copy()\n",
    "        expand_state = np.expand_dims(expand_state, axis=0).repeat(len(actions), axis=0)\n",
    "\n",
    "        expand_state, reward = self.muZero.dynamics(\n",
    "            torch.tensor(expand_state, dtype=torch.float32, device=self.muZero.device), actions)\n",
    "        expand_state = expand_state.detach().cpu().numpy()\n",
    "        expand_state = self.game.get_canonical_state(expand_state, -1).copy()\n",
    "        \n",
    "        for i, a in enumerate(actions):\n",
    "            child = Node(\n",
    "                expand_state[i],\n",
    "                reward[i],\n",
    "                action_probs[a],\n",
    "                self.muZero,\n",
    "                self.args,\n",
    "                self.game,\n",
    "                parent=self,\n",
    "                action_taken=a,\n",
    "            )\n",
    "            self.children.append(child)\n",
    "\n",
    "    def backpropagate(self, value):\n",
    "        self.total_value += value\n",
    "        self.visit_count += 1\n",
    "        if self.parent is not None:\n",
    "            self.parent.backpropagate(self.game.get_opponent_value(value))\n",
    "\n",
    "    def is_expandable(self):\n",
    "        return len(self.children) > 0\n",
    "\n",
    "    def select_child(self):\n",
    "        best_score = -np.inf\n",
    "        best_child = None\n",
    "\n",
    "        for child in self.children:\n",
    "            ucb_score = self.get_ucb_score(child)\n",
    "            if ucb_score > best_score:\n",
    "                best_score = ucb_score\n",
    "                best_child = child\n",
    "\n",
    "        return best_child\n",
    "\n",
    "    def get_ucb_score(self, child):\n",
    "        # prior_score = child.prior * math.sqrt(self.visit_count) / (1 + child.visit_count) * (self.args['c1'] + math.log((self.visit_count + self.args['c2'] + 1) / self.args['c2']))\n",
    "        prior_score = self.args['c'] * child.prior * math.sqrt(self.visit_count) / (1 + child.visit_count)\n",
    "        if child.visit_count == 0:\n",
    "            return prior_score\n",
    "        return prior_score - (child.total_value / child.visit_count)\n",
    "\n",
    "class MCTS:\n",
    "    def __init__(self, muZero, game, args):\n",
    "        self.muZero = muZero\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def search(self, state, reward, available_actions):\n",
    "        hidden_state = self.muZero.represent(\n",
    "            torch.tensor(state, dtype=torch.float32, device=self.muZero.device).unsqueeze(0)\n",
    "        )\n",
    "        action_probs, value = self.muZero.predict(hidden_state)\n",
    "        hidden_state = hidden_state.cpu().numpy().squeeze(0)\n",
    "        \n",
    "        root = Node(hidden_state, reward, 0, self.muZero, self.args, self.game)\n",
    "\n",
    "        action_probs = torch.softmax(action_probs, dim=1).cpu().numpy().squeeze(0)\n",
    "        action_probs = (1 - self.args['dirichlet_epsilon']) * action_probs + self.args['dirichlet_epsilon'] * np.random.dirichlet([self.args['dirichlet_alpha']] * self.game.action_size)\n",
    "        action_probs *= available_actions\n",
    "        action_probs /= np.sum(action_probs)\n",
    "\n",
    "        root.expand(action_probs)\n",
    "\n",
    "        for simulation in range(self.args['num_mcts_runs']):\n",
    "            node = root\n",
    "\n",
    "            while node.is_expandable():\n",
    "                node = node.select_child()\n",
    "\n",
    "            action_probs, value = self.muZero.predict(\n",
    "                torch.tensor(node.state, dtype=torch.float32, device=self.muZero.device).unsqueeze(0)\n",
    "            )\n",
    "            action_probs = torch.softmax(action_probs, dim=1).cpu().numpy().squeeze(0)\n",
    "            value = value.item()\n",
    "\n",
    "            node.expand(action_probs)\n",
    "            node.backpropagate(value)\n",
    "\n",
    "        return root\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MuZero(nn.Module):\n",
    "    def __init__(self, game):\n",
    "        super().__init__()\n",
    "        self.game = game\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.predictionFunction = PredictionFunction(self.game)\n",
    "        self.dynamicsFunction = DynamicsFunction()\n",
    "        self.representationFunction = RepresentationFunction()\n",
    "\n",
    "    def predict(self, hidden_state):\n",
    "        return self.predictionFunction(hidden_state)\n",
    "\n",
    "    def represent(self, observation):\n",
    "        return self.representationFunction(observation)\n",
    "\n",
    "    def dynamics(self, hidden_state, action):\n",
    "        actionArr = torch.zeros((hidden_state.shape[0], 2), device=self.device, dtype=torch.float32)\n",
    "        for i, a in enumerate(action):\n",
    "            actionArr[i, a] = 1\n",
    "        x = torch.hstack((hidden_state, actionArr))\n",
    "        return self.dynamicsFunction(x)\n",
    "\n",
    "# Creates hidden state + reward based on old hidden state and action \n",
    "class DynamicsFunction(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.startBlock = nn.Sequential(\n",
    "            nn.Linear(11, 48),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(48, 72),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(72, 156),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(156, 156),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(156, 72),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(72, 48),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(48, 9),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.rewardBlock = nn.Sequential(\n",
    "            nn.Linear(9, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.startBlock(x)\n",
    "        for block in self.resBlocks:\n",
    "            x = block(x)\n",
    "        x = self.endBlock(x)\n",
    "        reward = self.rewardBlock(x)\n",
    "        return x, reward\n",
    "    \n",
    "# Creates policy and value based on hidden state\n",
    "class PredictionFunction(nn.Module):\n",
    "    def __init__(self, game):\n",
    "        super().__init__()\n",
    "        self.game = game\n",
    "        \n",
    "        self.startBlock = nn.Sequential(\n",
    "            nn.Linear(9, 48),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(48, 72),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(72, 156),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(156, 156),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(156, 72),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(72, 32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.policy_head = nn.Sequential(\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, self.game.action_size)\n",
    "        )\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.startBlock(x)\n",
    "        p = self.policy_head(x)\n",
    "        v = self.value_head(x)\n",
    "        return p, v\n",
    "\n",
    "# Creates initial hidden state based on observation | several observations\n",
    "class RepresentationFunction(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.startBlock = nn.Sequential(\n",
    "            nn.Linear(4, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 48),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(48, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 9),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.startBlock(x)\n",
    "        return x\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1, stride=stride, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1, stride=stride, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += residual\n",
    "        out = F.relu(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, muZero, optimizer, game, args):\n",
    "        self.muZero = muZero\n",
    "        self.optimizer = optimizer\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.mcts = MCTS(self.muZero, self.game, self.args)\n",
    "        self.replayBuffer = ReplayBuffer(self.args, self.game)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def self_play(self, game_idx):\n",
    "        game_memory = []\n",
    "        player = 1\n",
    "        observation, valid_locations, reward, is_terminal = self.game.get_initial_state()\n",
    "\n",
    "        while True:\n",
    "            encoded_observation = self.game.get_encoded_observation(observation)\n",
    "            canonical_observation = self.game.get_canonical_state(encoded_observation, player).copy()\n",
    "            root = self.mcts.search(canonical_observation, reward, valid_locations)\n",
    "\n",
    "            action_probs = [0] * self.game.action_size\n",
    "            for child in root.children:\n",
    "                action_probs[child.action_taken] = child.visit_count\n",
    "            action_probs /= np.sum(action_probs)\n",
    "\n",
    "            # sample action from the mcts policy | based on temperature\n",
    "            if self.args['temperature'] == 0:\n",
    "                action = np.argmax(action_probs)\n",
    "            elif self.args['temperature'] == float('inf'):\n",
    "                action = np.random.choice([r for r in range(self.game.action_size) if action_probs[r] > 0])\n",
    "            else:\n",
    "                temperature_action_probs = action_probs ** (1 / self.args['temperature'])\n",
    "                temperature_action_probs /= np.sum(temperature_action_probs)\n",
    "                action = np.random.choice(len(temperature_action_probs), p=temperature_action_probs)\n",
    "\n",
    "            game_memory.append((canonical_observation, action, player, action_probs, reward, is_terminal))\n",
    "\n",
    "            observation, valid_locations, reward, is_terminal = self.game.step(action)\n",
    "\n",
    "            if is_terminal:\n",
    "                return_memory = []\n",
    "                for hist_state, hist_action, hist_player, hist_action_probs, hist_reward, hist_terminal in game_memory:\n",
    "                    return_memory.append((\n",
    "                        hist_state,\n",
    "                        hist_action, \n",
    "                        hist_action_probs,\n",
    "                        hist_reward,\n",
    "                        game_idx,\n",
    "                        hist_terminal\n",
    "                    ))\n",
    "                if not self.args['K'] > 0:\n",
    "                    return_memory.append((\n",
    "                        self.game.get_canonical_state(self.game.get_encoded_observation(observation), self.game.get_opponent_player(player)).copy(),\n",
    "                        None,\n",
    "                        np.zeros(self.game.action_size, dtype=np.float32),\n",
    "                        0,\n",
    "                        game_idx,\n",
    "                        is_terminal\n",
    "                    ))\n",
    "                return return_memory\n",
    "\n",
    "            player = self.game.get_opponent_player(player)\n",
    "\n",
    "    def train(self):\n",
    "        random.shuffle(self.replayBuffer.trajectories)\n",
    "        for batchIdx in range(0, len(self.replayBuffer) - 1, self.args['batch_size']): \n",
    "            policy_loss = 0\n",
    "            value_loss = 0\n",
    "            reward_loss = 0\n",
    "\n",
    "            observation, action, policy, value, reward = list(zip(*self.replayBuffer.trajectories[batchIdx:min(len(self.replayBuffer) -1, batchIdx + self.args['batch_size'])]))\n",
    "            observation = np.stack(observation)\n",
    "\n",
    "            state = torch.tensor(observation, dtype=torch.float32, device=self.device)\n",
    "            action = np.array(action).swapaxes(0, 1)\n",
    "            policy = torch.tensor(np.stack(policy).swapaxes(0, 1), dtype=torch.float32, device=self.device)\n",
    "            value = torch.tensor(np.expand_dims(np.array(value).swapaxes(0, 1), -1), dtype=torch.float32, device=self.device)\n",
    "            reward = torch.tensor(np.expand_dims(np.array(reward).swapaxes(0, 1), -1), dtype=torch.float32, device=self.device)\n",
    "\n",
    "            state = self.muZero.represent(state)\n",
    "            out_policy, out_value = self.muZero.predict(state)\n",
    "\n",
    "            policy_loss += F.cross_entropy(out_policy, policy[0]) \n",
    "            value_loss += F.mse_loss(out_value, value[0])\n",
    "\n",
    "            if self.args['K'] > 0:\n",
    "                for k in range(1, self.args['K'] + 1):\n",
    "                    state, out_reward = self.muZero.dynamics(state, action[k - 1])\n",
    "                    observation = state.detach().cpu().numpy()\n",
    "                    \n",
    "                    reward_loss += F.mse_loss(out_reward, reward[k])\n",
    "\n",
    "                    observation = self.game.get_canonical_state(observation, -1).copy()\n",
    "                    state = torch.tensor(observation, dtype=torch.float32, device=self.device)\n",
    "\n",
    "                    out_policy, out_value = self.muZero.predict(state)\n",
    "\n",
    "                    policy_loss += F.cross_entropy(out_policy, policy[k])\n",
    "                    value_loss += F.mse_loss(out_value, value[k])\n",
    "\n",
    "            loss = value_loss * self.args['value_loss_weight'] + policy_loss + reward_loss\n",
    "            loss /= self.args['K'] + 1\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "    def run(self):\n",
    "        for iteration in range(self.args['num_iterations']):\n",
    "            print(f\"iteration: {iteration}\")\n",
    "            self.replayBuffer.empty()\n",
    "\n",
    "            self.muZero.eval()\n",
    "            for train_game_idx in trange(self.args['num_train_games'], desc=\"train_game\"):\n",
    "                self.replayBuffer.memory += self.self_play(train_game_idx + iteration * self.args['num_train_games'])\n",
    "            self.replayBuffer.build_trajectories()\n",
    "\n",
    "            self.muZero.train()\n",
    "            for epoch in trange(self.args['num_epochs'], desc=\"epochs\"):\n",
    "                self.train()\n",
    "\n",
    "            torch.save(self.muZero.state_dict(), f\"Models/{self.game}/model_{iteration}.pt\")\n",
    "            torch.save(self.optimizer.state_dict(), f\"Models/{self.game}/optimizer_{iteration}.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f667e4baa0543019aab3f2c5ed406e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_game:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca0cf140ebe34267b208020bd3c042e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epochs:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aae00d94b0cc4bfb8c0ff2647087af8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_game:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/robert/Documents/GitHub/MuZero/cartPoleTest.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/robert/Documents/GitHub/MuZero/cartPoleTest.ipynb#W6sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(muZero\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/robert/Documents/GitHub/MuZero/cartPoleTest.ipynb#W6sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(muZero, optimizer, game, args)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/robert/Documents/GitHub/MuZero/cartPoleTest.ipynb#W6sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m trainer\u001b[39m.\u001b[39mrun()\n",
      "\u001b[1;32m/home/robert/Documents/GitHub/MuZero/cartPoleTest.ipynb Cell 7\u001b[0m in \u001b[0;36mTrainer.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/robert/Documents/GitHub/MuZero/cartPoleTest.ipynb#W6sZmlsZQ%3D%3D?line=112'>113</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmuZero\u001b[39m.\u001b[39meval()\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/robert/Documents/GitHub/MuZero/cartPoleTest.ipynb#W6sZmlsZQ%3D%3D?line=113'>114</a>\u001b[0m \u001b[39mfor\u001b[39;00m train_game_idx \u001b[39min\u001b[39;00m trange(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs[\u001b[39m'\u001b[39m\u001b[39mnum_train_games\u001b[39m\u001b[39m'\u001b[39m], desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain_game\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/robert/Documents/GitHub/MuZero/cartPoleTest.ipynb#W6sZmlsZQ%3D%3D?line=114'>115</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplayBuffer\u001b[39m.\u001b[39mmemory \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_play(train_game_idx \u001b[39m+\u001b[39;49m iteration \u001b[39m*\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs[\u001b[39m'\u001b[39;49m\u001b[39mnum_train_games\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/robert/Documents/GitHub/MuZero/cartPoleTest.ipynb#W6sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplayBuffer\u001b[39m.\u001b[39mbuild_trajectories()\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/robert/Documents/GitHub/MuZero/cartPoleTest.ipynb#W6sZmlsZQ%3D%3D?line=117'>118</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmuZero\u001b[39m.\u001b[39mtrain()\n",
      "\u001b[1;32m/home/robert/Documents/GitHub/MuZero/cartPoleTest.ipynb Cell 7\u001b[0m in \u001b[0;36mTrainer.self_play\u001b[0;34m(self, game_idx)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/robert/Documents/GitHub/MuZero/cartPoleTest.ipynb#W6sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m encoded_observation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgame\u001b[39m.\u001b[39mget_encoded_observation(observation)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/robert/Documents/GitHub/MuZero/cartPoleTest.ipynb#W6sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m canonical_observation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgame\u001b[39m.\u001b[39mget_canonical_state(encoded_observation, player)\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/robert/Documents/GitHub/MuZero/cartPoleTest.ipynb#W6sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m root \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmcts\u001b[39m.\u001b[39;49msearch(canonical_observation, reward, valid_locations)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/robert/Documents/GitHub/MuZero/cartPoleTest.ipynb#W6sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m action_probs \u001b[39m=\u001b[39m [\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgame\u001b[39m.\u001b[39maction_size\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/robert/Documents/GitHub/MuZero/cartPoleTest.ipynb#W6sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mfor\u001b[39;00m child \u001b[39min\u001b[39;00m root\u001b[39m.\u001b[39mchildren:\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m/home/robert/Documents/GitHub/MuZero/cartPoleTest.ipynb Cell 7\u001b[0m in \u001b[0;36mMCTS.search\u001b[0;34m(self, state, reward, available_actions)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/robert/Documents/GitHub/MuZero/cartPoleTest.ipynb#W6sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m     action_probs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msoftmax(action_probs, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/robert/Documents/GitHub/MuZero/cartPoleTest.ipynb#W6sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m     value \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39mitem()\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/robert/Documents/GitHub/MuZero/cartPoleTest.ipynb#W6sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m     node\u001b[39m.\u001b[39;49mexpand(action_probs)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/robert/Documents/GitHub/MuZero/cartPoleTest.ipynb#W6sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m     node\u001b[39m.\u001b[39mbackpropagate(value)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/robert/Documents/GitHub/MuZero/cartPoleTest.ipynb#W6sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m \u001b[39mreturn\u001b[39;00m root\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m/home/robert/Documents/GitHub/MuZero/cartPoleTest.ipynb Cell 7\u001b[0m in \u001b[0;36mNode.expand\u001b[0;34m(self, action_probs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/robert/Documents/GitHub/MuZero/cartPoleTest.ipynb#W6sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m expand_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/robert/Documents/GitHub/MuZero/cartPoleTest.ipynb#W6sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m expand_state \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexpand_dims(expand_state, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mrepeat(\u001b[39mlen\u001b[39m(actions), axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/robert/Documents/GitHub/MuZero/cartPoleTest.ipynb#W6sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m expand_state, reward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmuZero\u001b[39m.\u001b[39;49mdynamics(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/robert/Documents/GitHub/MuZero/cartPoleTest.ipynb#W6sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     torch\u001b[39m.\u001b[39;49mtensor(expand_state, dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat32, device\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmuZero\u001b[39m.\u001b[39;49mdevice), actions)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/robert/Documents/GitHub/MuZero/cartPoleTest.ipynb#W6sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m expand_state \u001b[39m=\u001b[39m expand_state\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/robert/Documents/GitHub/MuZero/cartPoleTest.ipynb#W6sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m expand_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgame\u001b[39m.\u001b[39mget_canonical_state(expand_state, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcopy()\n",
      "\u001b[1;32m/home/robert/Documents/GitHub/MuZero/cartPoleTest.ipynb Cell 7\u001b[0m in \u001b[0;36mMuZero.dynamics\u001b[0;34m(self, hidden_state, action)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/robert/Documents/GitHub/MuZero/cartPoleTest.ipynb#W6sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     actionArr[i, \u001b[39m0\u001b[39m, a] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/robert/Documents/GitHub/MuZero/cartPoleTest.ipynb#W6sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((hidden_state, actionArr), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/robert/Documents/GitHub/MuZero/cartPoleTest.ipynb#W6sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdynamicsFunction(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/robert/Documents/GitHub/MuZero/cartPoleTest.ipynb Cell 7\u001b[0m in \u001b[0;36mDynamicsFunction.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/robert/Documents/GitHub/MuZero/cartPoleTest.ipynb#W6sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstartBlock(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/robert/Documents/GitHub/MuZero/cartPoleTest.ipynb#W6sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39mfor\u001b[39;00m block \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresBlocks:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/robert/Documents/GitHub/MuZero/cartPoleTest.ipynb#W6sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     x \u001b[39m=\u001b[39m block(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/robert/Documents/GitHub/MuZero/cartPoleTest.ipynb#W6sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mendBlock(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/robert/Documents/GitHub/MuZero/cartPoleTest.ipynb#W6sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m reward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewardBlock(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/robert/Documents/GitHub/MuZero/cartPoleTest.ipynb Cell 7\u001b[0m in \u001b[0;36mResBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/robert/Documents/GitHub/MuZero/cartPoleTest.ipynb#W6sZmlsZQ%3D%3D?line=130'>131</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/robert/Documents/GitHub/MuZero/cartPoleTest.ipynb#W6sZmlsZQ%3D%3D?line=131'>132</a>\u001b[0m     residual \u001b[39m=\u001b[39m x\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/robert/Documents/GitHub/MuZero/cartPoleTest.ipynb#W6sZmlsZQ%3D%3D?line=132'>133</a>\u001b[0m     out \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbn1(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)))\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/robert/Documents/GitHub/MuZero/cartPoleTest.ipynb#W6sZmlsZQ%3D%3D?line=133'>134</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn2(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(out))\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/robert/Documents/GitHub/MuZero/cartPoleTest.ipynb#W6sZmlsZQ%3D%3D?line=134'>135</a>\u001b[0m     out \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m residual\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py:168\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    163\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m    169\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    170\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    171\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[1;32m    172\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[1;32m    173\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    175\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    176\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m    177\u001b[0m     bn_training,\n\u001b[1;32m    178\u001b[0m     exponential_average_factor,\n\u001b[1;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[1;32m    180\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/torch/nn/functional.py:2439\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2435\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[1;32m   2436\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[1;32m   2438\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mbatch_norm(\n\u001b[0;32m-> 2439\u001b[0m     \u001b[39minput\u001b[39m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[1;32m   2440\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/torch/backends/__init__.py:32\u001b[0m, in \u001b[0;36mContextProp.__get__\u001b[0;34m(self, obj, objtype)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__get__\u001b[39m(\u001b[39mself\u001b[39m, obj, objtype):\n\u001b[0;32m---> 32\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgetter()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    'num_iterations': 10,\n",
    "    'num_train_games': 100,\n",
    "    'num_mcts_runs': 50,\n",
    "    'num_epochs': 4,\n",
    "    'batch_size': 64,\n",
    "    'temperature': 1,\n",
    "    'K': 5,\n",
    "    'c': 2,\n",
    "    'N': 10,\n",
    "    'dirichlet_alpha': 0.3,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'gamma': 0.997,\n",
    "    'value_loss_weight': 0.25,\n",
    "}\n",
    "\n",
    "game = CartPole()\n",
    "muZero = MuZero(game).to(device)\n",
    "optimizer = torch.optim.Adam(muZero.parameters(), lr=0.001)\n",
    "trainer = Trainer(muZero, optimizer, game, args)\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make('CartPole-v1', render_mode=\"human\")\n",
    "testGame = CartPole()\n",
    "\n",
    "muZero = MuZero(testGame).to(device)\n",
    "muZero.load_state_dict(torch.load(\"Models/CartPole-v1/model_9.pt\", map_location=device))\n",
    "muZero.eval()\n",
    "\n",
    "observation, info = env.reset(seed=42)\n",
    "encoded_observation = testGame.get_encoded_observation(observation)\n",
    "encoded_observation = torch.tensor(encoded_observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "hidden_state = muZero.represent(encoded_observation)\n",
    "\n",
    "for _ in range(1000):\n",
    "   action_probs, value = muZero.predict(hidden_state)\n",
    "   action_probs = torch.softmax(action_probs, dim=1)\n",
    "   action = torch.argmax(action_probs).item()\n",
    "\n",
    "   observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "   if terminated or truncated:\n",
    "      observation, info = env.reset()\n",
    "\n",
    "   encoded_observation = testGame.get_encoded_observation(observation)\n",
    "   encoded_observation = torch.tensor(encoded_observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "   hidden_state = muZero.represent(encoded_observation)\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Mar 28 2022, 11:38:47) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2177f1ca12c1330a133c1d40b46100b268ab447cddcbdfdc0c7b2b7e4840e700"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
